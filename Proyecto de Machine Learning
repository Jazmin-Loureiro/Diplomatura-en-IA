Trabajo Práctio IA 2024


#Importamos las librerias que vamos a necesitar y el dataset.
import pandas as pd
import numpy as np
import io
import nltk
import re
import string
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB

#Seleccionamos el dataset.
from google.colab import files
uploaded = files.upload()

#Vemos las primeras 5 filas.
datost = pd.read_csv(io.BytesIO(uploaded['Dataset.txt']))
datost.head()

#Se observan la cantidad de observaciones y caracteristicas del dataset (filas y columnas).
datost.shape

#Vemos los nombres de todas las columnas del dataset.
datost.columns.values

#Eliminamos las columnas que no son necesarias para hacer menos pesado el dataset.
df = datost.drop(['confidence', 'social_timestamp', 'social_karma', 'syntax_ari',
       'lex_liwc_WC', 'lex_liwc_Analytic', 'lex_liwc_Clout',
       'lex_liwc_Authentic', 'lex_liwc_Tone', 'lex_liwc_WPS',
       'lex_liwc_Sixltr', 'lex_liwc_Dic', 'lex_liwc_function',
       'lex_liwc_pronoun', 'lex_liwc_ppron', 'lex_liwc_i', 'lex_liwc_we',
       'lex_liwc_you', 'lex_liwc_shehe', 'lex_liwc_they',
       'lex_liwc_ipron', 'lex_liwc_article', 'lex_liwc_prep',
       'lex_liwc_auxverb', 'lex_liwc_adverb', 'lex_liwc_conj',
       'lex_liwc_negate', 'lex_liwc_verb', 'lex_liwc_adj',
       'lex_liwc_compare', 'lex_liwc_interrog', 'lex_liwc_number',
       'lex_liwc_quant', 'lex_liwc_affect', 'lex_liwc_posemo',
       'lex_liwc_negemo', 'lex_liwc_anx', 'lex_liwc_anger',
       'lex_liwc_sad', 'lex_liwc_social', 'lex_liwc_family',
       'lex_liwc_friend', 'lex_liwc_female', 'lex_liwc_male',
       'lex_liwc_cogproc', 'lex_liwc_insight', 'lex_liwc_cause',
       'lex_liwc_discrep', 'lex_liwc_tentat', 'lex_liwc_certain',
       'lex_liwc_differ', 'lex_liwc_percept', 'lex_liwc_see',
       'lex_liwc_hear', 'lex_liwc_feel', 'lex_liwc_bio', 'lex_liwc_body',
       'lex_liwc_health', 'lex_liwc_sexual', 'lex_liwc_ingest',
       'lex_liwc_drives', 'lex_liwc_affiliation', 'lex_liwc_achieve',
       'lex_liwc_power', 'lex_liwc_reward', 'lex_liwc_risk',
       'lex_liwc_focuspast', 'lex_liwc_focuspresent',
       'lex_liwc_focusfuture', 'lex_liwc_relativ', 'lex_liwc_motion',
       'lex_liwc_space', 'lex_liwc_time', 'lex_liwc_work',
       'lex_liwc_leisure', 'lex_liwc_home', 'lex_liwc_money',
       'lex_liwc_relig', 'lex_liwc_death', 'lex_liwc_informal',
       'lex_liwc_swear', 'lex_liwc_netspeak', 'lex_liwc_assent',
       'lex_liwc_nonflu', 'lex_liwc_filler', 'lex_liwc_AllPunc',
       'lex_liwc_Period', 'lex_liwc_Comma', 'lex_liwc_Colon',
       'lex_liwc_SemiC', 'lex_liwc_QMark', 'lex_liwc_Exclam',
       'lex_liwc_Dash', 'lex_liwc_Quote', 'lex_liwc_Apostro',
       'lex_liwc_Parenth', 'lex_liwc_OtherP', 'lex_dal_max_pleasantness',
       'lex_dal_max_activation', 'lex_dal_max_imagery',
       'lex_dal_min_pleasantness', 'lex_dal_min_activation',
       'lex_dal_min_imagery', 'lex_dal_avg_activation',
       'lex_dal_avg_imagery', 'lex_dal_avg_pleasantness',
       'social_upvote_ratio', 'social_num_comments', 'syntax_fk_grade',
       'sentiment'], axis=1)

#Observamos el nuevo dataset.
df.head()

#Se observan la cantidad de observaciones y caracteristicas del nuevo dataset (filas y columnas).
df.shape

#Elimino 1000 filas de dataframe debido a que sino no es posible traducirlo para poder utilizarlo en espa;ol

# Seleccionar 1000 filas aleatorias para eliminar
rows_to_remove = np.random.choice(df.index, size=1000, replace=False)

# Eliminar las filas seleccionadas
df = df.drop(rows_to_remove).reset_index(drop=True)

#Se observan la cantidad de observaciones y caracteristicas del dataset (filas y columnas).
df.shape

#Se observan en detalle los datos.
df.head()

#Instalo googletrans and deep-translator para luego poder realizar la traduccion de los textos.
!pip install googletrans==4.0.0-rc1
!pip install deep-translator

#Traducimos la columna "text" ya que se quiere crear un modelo que analice textos en espa;ol.

from deep_translator import GoogleTranslator
from concurrent.futures import ThreadPoolExecutor, as_completed

# Crear una instancia del traductor
translator = GoogleTranslator(source='en', target='es')

# Tamaño del lote
batch_size = 1838  # Ajusta según sea necesario

# Función para traducir un lote de textos
def translate_batch(texts):
    translations = []
    for text in texts:
        try:
            translations.append(translator.translate(text))
        except Exception as e:
            print(f"Error al traducir el texto: {e}")
            translations.append(text)  # O puedes devolver un valor por defecto
    return translations

#Dividir la columna 'text' en lotes y traducir
texts = df['text']
translated_texts = []

def translate_in_parallel(texts, batch_size):
    futures = []
    with ThreadPoolExecutor(max_workers=30) as executor:  #Ajusta el número de trabajadores según sea necesario
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            futures.append(executor.submit(translate_batch, batch))
        for future in as_completed(futures):
            translated_texts.extend(future.result())

translate_in_parallel(texts, batch_size)

#Asignar las traducciones al DataFrame
df['text'] = translated_texts

#Vemos los textos traducidos.
df['text'].head()

#Nos fijamos si hay nulos en el dataset.
print(df.isnull().sum())

#Ahora limpiamos la columna de texto de palabras vacias, enlaces, simbolos especiales y errores del idioma.
from nltk.corpus import stopwords

# Descargar las stopwords de NLTK (si no lo has hecho ya)
nltk.download('stopwords')

# Definir el stemmer y las stopwords
stemmer = nltk.SnowballStemmer("spanish")
stopword = set(stopwords.words('spanish'))

# Definir la función de limpieza
def clean(text):
    text = str(text).lower()
    text = re.sub(r'\[.*?\]', '', text)  # Eliminar texto entre corchetes
    text = re.sub(r'https?://\S+|www\.\S+', '', text)  # Eliminar URLs
    text = re.sub(r'<.*?>+', '', text)  # Eliminar etiquetas HTML
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Eliminar puntuación
    text = re.sub(r'\n', '', text)  # Eliminar saltos de línea
    text = re.sub(r'\w*\d\w*', '', text)  # Eliminar palabras que contienen números
    text = [word for word in text.split(' ') if word not in stopword]  # Eliminar stopwords
    text = " ".join(text)
    text = [stemmer.stem(word) for word in text.split(' ')]  # Aplicar stemming
    text = " ".join(text)
    return text

# Aplicar la función de limpieza a la columna 'text'
df["text"] = df["text"].apply(clean)

#Ahora analizaremos las palabras mas mas utilizadas por
#las personas que comparten los problemas diarios de sus vidas en las redes sociales
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
text = " ".join(i for i in df.text)
stopwords = set(STOPWORDS)
wordcloud = WordCloud(stopwords=stopwords,
                      background_color="white").generate(text)
plt.figure( figsize=(15,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

#MODELO DE DETECCION DE ESTRES

#La columna de "label" de este conjunto de datos contiene las etiquetas 0 y 1.
#0 significa que no hay estrés y 1 significa que hay estrés.
#Utilizaremos las etiquetas "Hay estrés y "No hay estrés" en lugar de 1 y 0.
#Prepararemos esta columna en consecuencia y seleccionaremos las columnas de texto y etiqueta
#para el proceso de entrenamiento de un modelo de aprendizaje automático.
df["label"] = df["label"].map({0: "No hay estrés", 1: "Hay estrés"})
data = df[["text", "label"]]
data.head()

#Ahora dividiremos el dataset en training y test sets.
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

x = np.array(df["text"])
y = np.array(df["label"])

cv = CountVectorizer()
X = cv.fit_transform(x)
xtrain, xtest, ytrain, ytest = train_test_split(X, y,
                                                test_size=0.33,
                                                random_state=42)

 #Entrenaremos el modelo de detección de estrés.
from sklearn.naive_bayes import BernoulliNB
model = BernoulliNB()
model.fit(xtrain, ytrain)

#Ahora probamos el rendimiento de nuestro modelo en algunas oraciones
#aleatorias basadas en la salud mental.

#Prueba para "Hay estres" : A veces siento que necesito ayuda.
#Prueba para "No hay estres" : La gente necesita cuidar su salud mental.

user = input("Enter a Text: ")
data = cv.transform([user]).toarray()
output = model.predict(data)
print(output)
